{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Introduction to Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a set of exercises and along with discussions and explanations to supplement the experiments we will conduct throughout the course. **The goal of this first workshop is to get you comfortable with the basics of data exploration, statistical modeling, and quantifying uncertainties.** Next time, we will explore more details about how we can make inferences from our data using more complicated models and how to numerically implement them.\n",
    "\n",
    "Throughout this notebook you may encounter particular portions/exercises that you may not know how to do off the top of your head. This is normal and happens all the time in research, since you're often learning as much code as you need to get by for any particular project. If this happens, remember that Google, Stack Exchange forums, and package documentation sites are your friends! Please don't be afraid to fill up reams of tabs with searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you're coding, it's always a good idea to try and import any expected dependencies early on. I've included a few standard ones along with their typical aliases, as well as a few to guarantee compatibility between Python 2 and 3 (so it doesn't matter which one you're running).\n",
    "\n",
    "This might require installing a few packages. If so, try:\n",
    "\n",
    "`` conda install <package> ``\n",
    "\n",
    "or \n",
    "\n",
    "``pip install package``\n",
    "\n",
    "and reloading the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 2/3 compatibility\n",
    "from __future__ import print_function, division\n",
    "from six.moves import range\n",
    "\n",
    "# in case we want to call the terminal (e.g., `rm`)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# numerics\n",
    "import numpy as np\n",
    "\n",
    "# science-oriented utility functions\n",
    "import scipy\n",
    "\n",
    "# plotting\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot within the notebook instead of new windows\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll import some additional packages later, but it's always best practice to put them as early as possible so that you know right away if something isn't available when running your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to get some practice with the data collection process, so at this point we will be looking at a modeling the temperature calibration of a simple receiver. Using the setup in class, fill in the data table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature\n",
    "T_hot = ...  # room temperature\n",
    "T_cold = ...  # cooled with liquid nitrogen\n",
    "\n",
    "# Power measurements\n",
    "P_hot = ...  # array of measured power\n",
    "P_cold = ...  # array of measured power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to collect some extra observations (which we will use later). Please add those in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra power measurements\n",
    "P_hot_2 = ...\n",
    "P_cold_2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These observations define a set of $y$ values for two measured $x$ values, which we can use to derive a linear fit for the temperature. This is enough to get us started on the basics on statistical analysis.\n",
    "\n",
    "Before getting really quantitative, let's take a moment to examine our data in a bit more detail. Below, plot the data using `plt.plot` with the temperature on the x-axis and power on the y-axis. **Please make sure to add axis labels and a title and note appropriate units.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot power vs temperature\n",
    "fig = plt.figure(figsize=(10, 10))  # create a figure\n",
    "plt.plot(...)\n",
    "\n",
    "# \"prettify\" by adding labels\n",
    "\n",
    "# finalize\n",
    "plt.tight_layout()  # helps keep things neat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first things that you can do when fitting data is a process known as **\"chi by eye\"**...which is a cheeky way of saying just play around with the parameters until you get a fit that looks decent. You'd be surprised how close this can often get you to something more rigorous.\n",
    "\n",
    "**Add in a linear fit to the above plot following**\n",
    "\n",
    "$$ y = ax + b $$\n",
    "\n",
    "where $a$ is the slope and $b$ is the intercept. What does this relationship broadly imply? How well could we expect to do at $T=0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean and Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above method might get us close, it is by no means a rigorous result. To make this more robust, we need to quantify how the uncertainties in our measurements propagate to the uncertainties in our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the bedrock ways of quantifying uncertainty is to assume that the data are randomly generated from a **Normal** (Gaussian) distribution with some mean $\\mu$ and standard deviation $\\sigma$. It can be shown that the best guess for the mean and standard deviation from a set of noisy observations is\n",
    "\n",
    "$$ \\mu = \\frac{1}{n} \\sum_{i=1}^{n} y_i $$ \n",
    "\n",
    "and \n",
    "\n",
    "$$ \\sigma = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\mu)^2} $$\n",
    "\n",
    "**Compute the mean and standard deviation for the measured powers \"manually\" and using convenience functions in `numpy`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct implementation\n",
    "n = ...  # number of objects\n",
    "P_hot_mean = ...\n",
    "P_hot_std = ...\n",
    "P_cold_mean = ...\n",
    "P_cold_std = ...\n",
    "\n",
    "# with convenience functions\n",
    "P_hot_mean2 = ...\n",
    "P_hot_std2 = ...\n",
    "P_cold_mean2 = ...\n",
    "P_cold_std2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note there's a slight disagreement between the value from `numpy` and the analytic solution, which is based upon whether we divide by $n$ or $n-1$. The former case is more precise but biased, while the latter is less precise but unbiased. We will just multiply our result by $\\sqrt{(n-1)/n}$ for now to stay consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_hot_std *= np.sqrt((n - 1) / n)\n",
    "P_cold_std *= np.sqrt((n - 1) / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are one of the ways in which astronomers try to quantify uncertainties. In other words, we are making a statement here about how likely it is that we would expect to see a new measurement $y_{\\rm new}$ based on our current measurements $y$ assuming our measurements follow the assumptions we made. Let's explore a few ways we can quantify this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-sigma deviations and Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common way a lot of researchers talk about uncertainties is talking about \"how many sigma\" an observation is away from the expected value. This can give a rough estimate of how likely/unlikely we would be to observed it.\n",
    "\n",
    "The cell below shows what this looks like for a few sigma. See if you can parse through each segment of the code to broadly understand what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate normal PDF, i.e. probability of\n",
    "# observation as a function of sigma deviations\n",
    "# away from the mean\n",
    "from scipy import stats  # statistics utilities\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "y = stats.norm.pdf(x)\n",
    "\n",
    "# plot N-sigma intervals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, y, lw=3, color='blue')\n",
    "for sigma in [1, 2, 3, 4, 5]:\n",
    "    plt.fill_between(x, y, where=(np.abs(x) <= sigma), \n",
    "                     alpha=0.3/sigma, color='blue')\n",
    "plt.xticks(np.arange(-5, 6))\n",
    "plt.xlabel('Sigma')\n",
    "plt.ylabel('Probability of Measurement')\n",
    "plt.title('Gaussian')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above plot shows, each $\\pm N \\sigma$ bound corresponds to some portion of the total probability of observing the next measurement. To put this another way, there is an $X$% chance that the next observation will be between $\\pm N$ standard deviations from the mean. This defines what is known as a **Confidence Interval**, which says how \"confident\" we are that the next observation will be between $\\mu - N\\sigma$ and $\\mu + N\\sigma$.\n",
    "\n",
    "Using your knowledge of Gaussians and/or Google, **define the 2-sigma 95% confidence intervals for the two temperature measurements** (i.e. 95% of observations are within $\\pm 2\\sigma$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define confidence intervals for P_hot\n",
    "...\n",
    "\n",
    "# define confidence intervals for P_cold\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify this result by using some numerical simulation by generating random data using `numpy.random`. We should hopefully see that the number of observations within our $N$-sigma bounds agree with what we'd expect from what we computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate normally-distributed random numbers\n",
    "n_rand = 100000\n",
    "P_hot_rand = np.random.normal(...)  # random realizations of P_hot\n",
    "P_cold_rand = np.random.normal(...)  # random realizations of P_cold\n",
    "\n",
    "# check fraction of observations within confidence intervals\n",
    "# sketch of example below\n",
    "n_hot_2sig = np.sum((P_hot_rand >= P_2siglow) & \n",
    "                     (P_hot_rand < P_2sighot))  # select within 1-sigma\n",
    "f_hot_2sig = n_hot_2sig / n_rand  # compute fraction\n",
    "print(f_hot_2sig, 0.954499736103642)  # print result vs truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p-values and Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to look at the result from above is in terms of **hypothesis testing**. In our calculations above, we have assumed that our model for the data (using the mean $\\mu$ and standard deviation $\\sigma$) is correct, and use that model to make predictions about what we think the next data will look like. But what if our model is wrong? One way to find out is to compare how well the next measurement agrees with our model. If it's extremely improbable under our current hypothesis, then our hypothesis is likely wrong and we need to update our model.\n",
    "\n",
    "Our way to quantify this is using **p-values**, which is the probability that you would see an observation *at least as extreme* as the one you observe. This is just the flip-side of the confidence interval. An illustration is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot p-value example\n",
    "obs = 1.7  # offset (in sigma) of measurement from mean\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, y, lw=3, color='blue')\n",
    "plt.vlines(obs, 0, max(y), colors='red', lw=2)\n",
    "plt.fill_between(x, y, where=np.abs(x) >= abs(obs), \n",
    "                 alpha=0.6, color='blue')\n",
    "plt.xticks(np.arange(-5, 6))\n",
    "plt.xlabel('Sigma')\n",
    "plt.ylabel('Probability of Measurement')\n",
    "plt.title('Gaussian')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the p-value is small enough, then we should reject our current model and try to pick something different. Typical values include $p=0.05$, $p=0.01$, and $p=0.001$. I personally like the latter since I'm very conservative about trying to assume new models without ample evidence for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we now want to check whether our extra observations from earlier imply that our assumptions for the mean and standard deviations for $P_{\\rm hot}$ and $P_{\\rm cold}$ are justified.\n",
    "\n",
    "**Compute the corresponding p-values for the extra observations above** by computing the fraction of simulated power measurements from earlier whose deviations from the mean exceed the deviation of the measured values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define normalized residuals\n",
    "P_hot_rand_sdev = np.abs(P_hot_rand - P_hot_mean) / P_hot_std\n",
    "P_cold_rand_sdev = np.abs(P_cold_rand - P_cold_mean) / P_cold_std\n",
    "\n",
    "# p-values for extra P_hot measurement (pick one)\n",
    "...\n",
    "\n",
    "# p-values for extra P_cold measurement (pick one)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have time and are interested in doing this more precisely, see if you can figure out how to compute the exact values from `scipy.stats.norm` using the [online documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy.stats.norm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining Uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we better understand the uncertainties from our measurement (and the limitations of our current approximations), we now want to figure out how to propagate those errors into our temperature calibration. As a first step, plot the noisy data using `plt.errorbar`. Feel free to play around with the options to get the results to look nice, and always remember to label your axes and plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot power vs temperature with errors\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.errorbar(...)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best-fit Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we're going to go through several iterations of propagating our errors into our results, based on a few different methods. First, we see that the slope of a line from two datapoints is just\n",
    "\n",
    "$$ a = \\frac{P_{\\rm hot} - P_{\\rm cold}}{T_{\\rm hot} - T_{\\rm cold}} $$\n",
    "\n",
    "and the intercept is\n",
    "\n",
    "$$ b = \\frac{P_{\\rm cold}T_{\\rm hot} - P_{\\rm hot}T_{\\rm cold}}{T_{\\rm hot} - T_{\\rm cold}} $$\n",
    "\n",
    "**Using these relations, compute the best-fit line and overplot it over the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best-fit line\n",
    "a_best = ...\n",
    "b_best = ...\n",
    "\n",
    "# plot power vs temperature with errors\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.errorbar(...)\n",
    "plt.plot(...)  # best-fit line\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare to the results you got by doing \"chi by eye\"? Are you surprised?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Error Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"simplest\" way to propagate errors is to essentially engage in a really simple thought experiment. Let's say that $P_{\\rm hot}$ changed by a tiny amount $\\Delta P_{\\rm hot}$. That would shift $a$ by a corresponding amount $\\Delta a$. Making $\\Delta$ infinitely small implies that this difference should depend on the derivative of $a$ with respect to $P_{\\rm hot}$. Likewise for $P_{\\rm cold}$. We can formalize this with the basic result:\n",
    "\n",
    "$$ \\sigma_f^2 = \\left|\\frac{\\partial f}{\\partial x} \\sigma_x\\right|^2 + \\left|\\frac{\\partial f}{\\partial x} \\sigma_y\\right|^2 + \\dots $$\n",
    "\n",
    "In other words, the errors $\\sigma_f$ on $f$ are just related to the errors on the individual parameters $x, y, \\dots$ multiplied by how sensitive $f$ is to each of them.\n",
    "\n",
    "**Using this result, compute the errors on $a$ and $b$ based on the measured errors in $P_{\\rm hot}$ and $P_{\\rm cold}$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivatives\n",
    "da_dp_hot = ...\n",
    "da_dp_cold = ...\n",
    "db_dp_hot = ...\n",
    "db_dp_cold = ...\n",
    "\n",
    "# errors in slope\n",
    "a_err_naive = \n",
    "\n",
    "# errors in intercept\n",
    "b_err_naive = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, generate 10 realizations of the fitted line based on these naive errors and overplot them over the data. What do you see? Do these uncertainties accurately capture the expected behavior? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot power vs temperature with errors\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.errorbar(...)\n",
    "plt.plot(...)  # best-fit line\n",
    "for i in range(10):\n",
    "    plt.plot(...)  # realizations of fit\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to look at this is to see the 2-D distribution of slopes vs intercepts. **Simulate 10000 slopes/intercepts from the naive errors and plot their distribution using `plt.hist2d`.** What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot slope vs intercept distribution\n",
    "plt.figure(figsize=(12, 10))\n",
    "a_draws = ...\n",
    "b_draws = ...\n",
    "plt.hist2d(...)\n",
    "plt.colorbar(label=...)  # add colorbar\n",
    "...  # add axis labels and title\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above is a great way to take a stab at the uncertainties, the real quantification of uncertainties can be substantially more complicated. For instance, we've measured the uncertainties here assuming that the mean and error that we've derived from our small set of observations accurately characterizes the true underlying measurement noise in the power from the receiver. This seems, at best, a stretch. Maybe it'd be a bit less of a stretch if we have many more observations though.\n",
    "\n",
    "There are also other possible errors that could impact our analysis. Maybe there's **systematic errors** (i.e. something we're doing consistently wrong) with the way we're measuring the temperature, which we're currently not taking into account. Or maybe there's some criteria we've imposed as part of our experiment that affects what measurements we take. While effects like these can sometimes be hard to include in simple analytic models like the one we specified above, they can naturally be explored from simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Sampling Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first want to run a simulation that ignores the impact of our sampling error and assumes that our measured means and standard deviations for the power is correct. We can then simulate the effect these have on our fitted relation directly by:\n",
    "\n",
    "\n",
    "1. Simulating new values for $P_{\\rm hot}$ and $P_{\\rm cold}$.\n",
    "2. Fitting a line based on the analytic relationship above.\n",
    "3. Repeat 1-2 a large number of times.\n",
    "4. Analyze the fitted slopes and intercepts.\n",
    "\n",
    "Let's implement this below. **Simulate 10000 realizations of the best-fit line and save the resulting slopes/intercepts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_arr, b_arr = ...\n",
    "\n",
    "for i in range(10000):\n",
    "    # simulate data\n",
    "    P_hot_r, P_cold_r = np.random.normal(...), np.random.normal(...)\n",
    "    \n",
    "    # fit line\n",
    "    a_r, b_r = ...\n",
    "    \n",
    "    # save fit\n",
    "    a_arr[i], b_arr[i] = a_r, b_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these compare to the values derived from our naive error analysis? **Plot the corresponding 2-D distributions for both analyses below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare simple errors vs numerical simulation\n",
    "plt.figure(figsize=(24, 10))\n",
    "\n",
    "# naive errors\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist2d(...)  # slope vs intercept\n",
    "plt.colorbar(label=...)  # add colorbar\n",
    "...  # add axis labels and title\n",
    "\n",
    "# numerical simulation\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist2d(...)  # slope vs intercept\n",
    "plt.colorbar(label=...)  # add colorbar\n",
    "...  # add axis labels and title\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Sampling Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to include the effect of sampling error, i.e. that we estimate the mean and variance from a small number of points. Here, let's assume that the means and errors we derive are correct, and analyze how following our analysis \"end to end\" introduces additional errors. This procedure now looks like:\n",
    "\n",
    "1. Simulate $n$ new values for $P_{\\rm hot}$ and $P_{\\rm cold}$.\n",
    "2. Compute the mean and standard deviation from the corresponding samples. This is analagous to what we measured.\n",
    "3. Simulate a *new* value for $P_{\\rm hot}$ and $P_{\\rm cold}$ from the mean and standard deviation derived above (instead of the one we started with).\n",
    "4. Fit a line based on the analytic relationship above.\n",
    "5. Repeat 1-4 a large number of times.\n",
    "6. Analyze the fitted slopes and intercepts.\n",
    "\n",
    "Let's implement this below. **Simulate 10000 realizations of the best-fit line incorporating sampling uncertainty and save the resulting slopes/intercepts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_arr2, b_arr2 = ...\n",
    "\n",
    "for i in range(10000):\n",
    "    # simulate n datapoints\n",
    "    P_hot_rs, P_cold_rs = np.random.normal(...), np.random.normal(...)\n",
    "    \n",
    "    # compute sample mean and standard deviation\n",
    "    P_hot_mean_r, P_cold_mean_r = ..., ...\n",
    "    P_hot_std_r, P_cold_std_r = ..., ...\n",
    "    \n",
    "    # simulate new datapoint\n",
    "    P_hot_r, P_cold_r = np.random.normal(...), np.random.normal(...)\n",
    "\n",
    "    \n",
    "    # fit line with `P_hot_r` and `P_cold_r`\n",
    "    a_r, b_r = ...\n",
    "    \n",
    "    # save fit\n",
    "    a_arr2[i], b_arr2[i] = a_r, b_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these compare to the values derived from our previous numerical simulation where we ignore this effect? **Plot the corresponding 2-D distributions below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare numerical results w/ and w/o sampling noise\n",
    "plt.figure(figsize=(24, 10))\n",
    "\n",
    "# it's important to have the same bins\n",
    "# so that the plots are to scale\n",
    "bins = [np.linspace(a_best - 5 * a_err_naive, a_best + 5 * a_err_naive, 100),\n",
    "        np.linspace(b_best - 5 * b_err_naive, b_best + 5 * b_err_naive, 100)]\n",
    "\n",
    "# w/o sampling noise\n",
    "# make sure to set `bins=bins` and `cmin=1` in `hist2d`\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist2d(...)  # slope vs intercept\n",
    "plt.colorbar(label=...)  # add colorbar\n",
    "...  # add axis labels and title\n",
    "\n",
    "# w/ sampling noise\n",
    "# make sure to set `bins=bins` and `cmin=1` in `hist2d`\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist2d(...)  # slope vs intercept\n",
    "plt.colorbar(label=...)  # add colorbar\n",
    "...  # add axis labels and title\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These *look* pretty similar, but one way to quantify the differences is to compute the **covariance** among these realizations and see how they differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute covariances\n",
    "cov_wosamp = np.cov(np.c_[a_arr, b_arr], rowvar=False)\n",
    "cov_wsamp = np.cov(np.c_[a_arr2, b_arr2], rowvar=False)\n",
    "\n",
    "print('Without sampling noise:')\n",
    "print(cov_wosamp)\n",
    "print('With sampling noise:')\n",
    "print(cov_wsamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next time, we'll start exploring more detail about how to interpret these simulation results in a more rigorous statistical context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
